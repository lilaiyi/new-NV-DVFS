
Fatbin elf code:
================
arch = sm_80
code version = [1,7]
producer = <unknown>
host = linux
compile_size = 64bit

Fatbin ptx code:
================
arch = sm_80
code version = [7,2]
producer = <unknown>
host = linux
compile_size = 64bit
compressed








.version 7.2
.target sm_80
.address_size 64

.const .align 4 .b8 c_Table[372];

.entry _Z26quasirandomGeneratorKernelPfjj(
.param .u64 _Z26quasirandomGeneratorKernelPfjj_param_0,
.param .u32 _Z26quasirandomGeneratorKernelPfjj_param_1,
.param .u32 _Z26quasirandomGeneratorKernelPfjj_param_2
)
{
.reg .pred %p<37>;
.reg .b16 %rs<2>;
.reg .f32 %f<3>;
.reg .b32 %r<176>;
.reg .b64 %rd<8>;


ld.param.u64 %rd3, [_Z26quasirandomGeneratorKernelPfjj_param_0];
ld.param.u32 %r70, [_Z26quasirandomGeneratorKernelPfjj_param_1];
ld.param.u32 %r71, [_Z26quasirandomGeneratorKernelPfjj_param_2];
mov.u32 %r72, %ntid.x;
and.b32 %r1, %r72, 16777215;
mov.u32 %r73, %ctaid.x;
and.b32 %r74, %r73, 16777215;
mov.u32 %r75, %tid.x;
mad.lo.s32 %r144, %r74, %r1, %r75;
setp.ge.u32 %p1, %r144, %r71;
@%p1 bra LBB0_65;

mov.u32 %r76, %tid.y;
and.b32 %r77, %r76, 16777215;
and.b32 %r78, %r71, 16777215;
mul.lo.s32 %r3, %r78, %r77;
mul.wide.u32 %rd4, %r76, 124;
mov.u64 %rd5, c_Table;
add.s64 %rd1, %rd5, %rd4;
mov.u32 %r79, %nctaid.x;
and.b32 %r80, %r79, 16777215;
mul.lo.s32 %r4, %r80, %r1;
cvta.to.global.u64 %rd2, %rd3;
mov.pred %p3, 0;

LBB0_2:
add.s32 %r6, %r144, %r70;
and.b32 %r82, %r6, 1;
setp.eq.b32 %p2, %r82, 1;
xor.pred %p4, %p2, %p3;
not.pred %p5, %p4;
mov.u32 %r146, 0;
@%p5 bra LBB0_4;

ld.const.u32 %r146, [%rd1];

LBB0_4:
and.b32 %r83, %r6, 2;
setp.eq.s32 %p6, %r83, 0;
@%p6 bra LBB0_6;

ld.const.u32 %r84, [%rd1+4];
xor.b32 %r146, %r84, %r146;

LBB0_6:
and.b32 %r85, %r6, 4;
setp.eq.s32 %p7, %r85, 0;
@%p7 bra LBB0_8;

ld.const.u32 %r86, [%rd1+8];
xor.b32 %r146, %r86, %r146;

LBB0_8:
and.b32 %r87, %r6, 8;
setp.eq.s32 %p8, %r87, 0;
@%p8 bra LBB0_10;

ld.const.u32 %r88, [%rd1+12];
xor.b32 %r146, %r88, %r146;

LBB0_10:
and.b32 %r89, %r6, 16;
setp.eq.s32 %p9, %r89, 0;
@%p9 bra LBB0_12;

ld.const.u32 %r90, [%rd1+16];
xor.b32 %r146, %r90, %r146;

LBB0_12:
and.b32 %r91, %r6, 32;
setp.eq.s32 %p10, %r91, 0;
@%p10 bra LBB0_14;

ld.const.u32 %r92, [%rd1+20];
xor.b32 %r146, %r92, %r146;

LBB0_14:
and.b32 %r93, %r6, 64;
setp.eq.s32 %p11, %r93, 0;
@%p11 bra LBB0_16;

ld.const.u32 %r94, [%rd1+24];
xor.b32 %r146, %r94, %r146;

LBB0_16:
and.b32 %r95, %r6, 128;
setp.eq.s32 %p12, %r95, 0;
@%p12 bra LBB0_18;

ld.const.u32 %r96, [%rd1+28];
xor.b32 %r146, %r96, %r146;

LBB0_18:
and.b32 %r97, %r6, 256;
setp.eq.s32 %p13, %r97, 0;
@%p13 bra LBB0_20;

ld.const.u32 %r98, [%rd1+32];
xor.b32 %r146, %r98, %r146;

LBB0_20:
and.b32 %r99, %r6, 512;
setp.eq.s32 %p14, %r99, 0;
@%p14 bra LBB0_22;

ld.const.u32 %r100, [%rd1+36];
xor.b32 %r146, %r100, %r146;

LBB0_22:
and.b32 %r101, %r6, 1024;
setp.eq.s32 %p15, %r101, 0;
@%p15 bra LBB0_24;

ld.const.u32 %r102, [%rd1+40];
xor.b32 %r146, %r102, %r146;

LBB0_24:
and.b32 %r103, %r6, 2048;
setp.eq.s32 %p16, %r103, 0;
@%p16 bra LBB0_26;

ld.const.u32 %r104, [%rd1+44];
xor.b32 %r146, %r104, %r146;

LBB0_26:
and.b32 %r105, %r6, 4096;
setp.eq.s32 %p17, %r105, 0;
@%p17 bra LBB0_28;

ld.const.u32 %r106, [%rd1+48];
xor.b32 %r146, %r106, %r146;

LBB0_28:
and.b32 %r107, %r6, 8192;
setp.eq.s32 %p18, %r107, 0;
@%p18 bra LBB0_30;

ld.const.u32 %r108, [%rd1+52];
xor.b32 %r146, %r108, %r146;

LBB0_30:
and.b32 %r109, %r6, 16384;
setp.eq.s32 %p19, %r109, 0;
@%p19 bra LBB0_32;

ld.const.u32 %r110, [%rd1+56];
xor.b32 %r146, %r110, %r146;

LBB0_32:
cvt.u16.u32 %rs1, %r6;
setp.gt.s16 %p20, %rs1, -1;
@%p20 bra LBB0_34;

ld.const.u32 %r111, [%rd1+60];
xor.b32 %r146, %r111, %r146;

LBB0_34:
and.b32 %r112, %r6, 65536;
setp.eq.s32 %p21, %r112, 0;
@%p21 bra LBB0_36;

ld.const.u32 %r113, [%rd1+64];
xor.b32 %r146, %r113, %r146;

LBB0_36:
and.b32 %r114, %r6, 131072;
setp.eq.s32 %p22, %r114, 0;
@%p22 bra LBB0_38;

ld.const.u32 %r115, [%rd1+68];
xor.b32 %r146, %r115, %r146;

LBB0_38:
and.b32 %r116, %r6, 262144;
setp.eq.s32 %p23, %r116, 0;
@%p23 bra LBB0_40;

ld.const.u32 %r117, [%rd1+72];
xor.b32 %r146, %r117, %r146;

LBB0_40:
and.b32 %r118, %r6, 524288;
setp.eq.s32 %p24, %r118, 0;
@%p24 bra LBB0_42;

ld.const.u32 %r119, [%rd1+76];
xor.b32 %r146, %r119, %r146;

LBB0_42:
and.b32 %r120, %r6, 1048576;
setp.eq.s32 %p25, %r120, 0;
@%p25 bra LBB0_44;

ld.const.u32 %r121, [%rd1+80];
xor.b32 %r146, %r121, %r146;

LBB0_44:
and.b32 %r122, %r6, 2097152;
setp.eq.s32 %p26, %r122, 0;
@%p26 bra LBB0_46;

ld.const.u32 %r123, [%rd1+84];
xor.b32 %r146, %r123, %r146;

LBB0_46:
and.b32 %r124, %r6, 4194304;
setp.eq.s32 %p27, %r124, 0;
@%p27 bra LBB0_48;

ld.const.u32 %r125, [%rd1+88];
xor.b32 %r146, %r125, %r146;

LBB0_48:
and.b32 %r126, %r6, 8388608;
setp.eq.s32 %p28, %r126, 0;
@%p28 bra LBB0_50;

ld.const.u32 %r127, [%rd1+92];
xor.b32 %r146, %r127, %r146;

LBB0_50:
and.b32 %r128, %r6, 16777216;
setp.eq.s32 %p29, %r128, 0;
@%p29 bra LBB0_52;

ld.const.u32 %r129, [%rd1+96];
xor.b32 %r146, %r129, %r146;

LBB0_52:
and.b32 %r130, %r6, 33554432;
setp.eq.s32 %p30, %r130, 0;
@%p30 bra LBB0_54;

ld.const.u32 %r131, [%rd1+100];
xor.b32 %r146, %r131, %r146;

LBB0_54:
and.b32 %r132, %r6, 67108864;
setp.eq.s32 %p31, %r132, 0;
@%p31 bra LBB0_56;

ld.const.u32 %r133, [%rd1+104];
xor.b32 %r146, %r133, %r146;

LBB0_56:
and.b32 %r134, %r6, 134217728;
setp.eq.s32 %p32, %r134, 0;
@%p32 bra LBB0_58;

ld.const.u32 %r135, [%rd1+108];
xor.b32 %r146, %r135, %r146;

LBB0_58:
and.b32 %r136, %r6, 268435456;
setp.eq.s32 %p33, %r136, 0;
@%p33 bra LBB0_60;

ld.const.u32 %r137, [%rd1+112];
xor.b32 %r146, %r137, %r146;

LBB0_60:
and.b32 %r138, %r6, 536870912;
setp.eq.s32 %p34, %r138, 0;
@%p34 bra LBB0_62;

ld.const.u32 %r139, [%rd1+116];
xor.b32 %r146, %r139, %r146;

LBB0_62:
and.b32 %r140, %r6, 1073741824;
setp.eq.s32 %p35, %r140, 0;
@%p35 bra LBB0_64;

ld.const.u32 %r141, [%rd1+120];
xor.b32 %r146, %r141, %r146;

LBB0_64:
add.s32 %r142, %r146, 1;
cvt.rn.f32.u32 %f1, %r142;
mul.f32 %f2, %f1, 0f30000000;
add.s32 %r143, %r144, %r3;
mul.wide.u32 %rd6, %r143, 4;
add.s64 %rd7, %rd2, %rd6;
st.global.f32 [%rd7], %f2;
add.s32 %r144, %r144, %r4;
setp.lt.u32 %p36, %r144, %r71;
@%p36 bra LBB0_2;

LBB0_65:
ret;

}
.entry _Z16inverseCNDKernelPfPjj(
.param .u64 _Z16inverseCNDKernelPfPjj_param_0,
.param .u64 _Z16inverseCNDKernelPfPjj_param_1,
.param .u32 _Z16inverseCNDKernelPfPjj_param_2
)
{
.reg .pred %p<10>;
.reg .f32 %f<61>;
.reg .b32 %r<28>;
.reg .b64 %rd<12>;


ld.param.u64 %rd5, [_Z16inverseCNDKernelPfPjj_param_0];
ld.param.u64 %rd4, [_Z16inverseCNDKernelPfPjj_param_1];
ld.param.u32 %r10, [_Z16inverseCNDKernelPfPjj_param_2];
cvta.to.global.u64 %rd1, %rd5;
mov.u32 %r11, %ntid.x;
and.b32 %r12, %r11, 16777215;
mov.u32 %r13, %ctaid.x;
and.b32 %r14, %r13, 16777215;
mov.u32 %r15, %tid.x;
mad.lo.s32 %r26, %r14, %r12, %r15;
mov.u32 %r16, %nctaid.x;
and.b32 %r17, %r16, 16777215;
mul.lo.s32 %r2, %r17, %r12;
setp.eq.s64 %p1, %rd4, 0;
@%p1 bra LBB1_7;

setp.ge.u32 %p2, %r26, %r10;
@%p2 bra LBB1_13;

cvta.to.global.u64 %rd2, %rd4;

LBB1_3:
cvt.u64.u32 %rd3, %r26;
mul.wide.u32 %rd6, %r26, 4;
add.s64 %rd7, %rd2, %rd6;
ld.global.u32 %r4, [%rd7];
shr.s32 %r18, %r4, 31;
xor.b32 %r19, %r18, %r4;
cvt.rn.f32.u32 %f11, %r19;
fma.rn.f32 %f1, %f11, 0f2F800000, 0f2F000000;
add.f32 %f2, %f1, 0fBF000000;
setp.gt.f32 %p3, %f2, 0fBED70A3D;
@%p3 bra LBB1_5;
bra.uni LBB1_4;

LBB1_5:
mul.f32 %f24, %f2, %f2;
fma.rn.f32 %f25, %f24, 0fC1CB874B, 0f42259096;
fma.rn.f32 %f26, %f24, %f25, 0fC194EB85;
fma.rn.f32 %f27, %f24, %f26, 0f40206C99;
mul.f32 %f28, %f2, %f27;
fma.rn.f32 %f29, %f24, 0f40485F81, 0fC1A87F78;
fma.rn.f32 %f30, %f24, %f29, 0f41B8AABD;
fma.rn.f32 %f31, %f24, %f30, 0fC1079380;
fma.rn.f32 %f32, %f24, %f31, 0f3F800000;
div.rn.f32 %f59, %f28, %f32;
bra.uni LBB1_6;

LBB1_4:
lg2.approx.f32 %f12, %f1;
mul.f32 %f13, %f12, 0fBF317218;
lg2.approx.f32 %f14, %f13;
mul.f32 %f15, %f14, 0f3F317218;
fma.rn.f32 %f16, %f15, 0f34D49E28, 0f349B0EAC;
fma.rn.f32 %f17, %f15, %f16, 0f3806F590;
fma.rn.f32 %f18, %f15, %f17, 0f39CF3175;
fma.rn.f32 %f19, %f15, %f18, 0f3B7BB21F;
fma.rn.f32 %f20, %f15, %f19, 0f3CE2756C;
fma.rn.f32 %f21, %f15, %f20, 0f3E24A839;
fma.rn.f32 %f22, %f15, %f21, 0f3F79E636;
fma.rn.f32 %f23, %f15, %f22, 0f3EACC996;
neg.f32 %f59, %f23;

LBB1_6:
neg.f32 %f33, %f59;
setp.lt.s32 %p4, %r4, 0;
selp.f32 %f34, %f33, %f59, %p4;
shl.b64 %rd8, %rd3, 2;
add.s64 %rd9, %rd1, %rd8;
st.global.f32 [%rd9], %f34;
cvt.u32.u64 %r20, %rd3;
add.s32 %r26, %r20, %r2;
setp.lt.u32 %p5, %r26, %r10;
@%p5 bra LBB1_3;
bra.uni LBB1_13;

LBB1_7:
setp.ge.u32 %p6, %r26, %r10;
@%p6 bra LBB1_13;

add.s32 %r21, %r10, 1;
mov.u32 %r22, -1;
div.u32 %r6, %r22, %r21;

LBB1_9:
add.s32 %r23, %r26, 1;
mul.lo.s32 %r8, %r23, %r6;
shr.s32 %r24, %r8, 31;
xor.b32 %r25, %r24, %r8;
cvt.rn.f32.u32 %f35, %r25;
fma.rn.f32 %f6, %f35, 0f2F800000, 0f2F000000;
add.f32 %f7, %f6, 0fBF000000;
setp.gt.f32 %p7, %f7, 0fBED70A3D;
@%p7 bra LBB1_11;
bra.uni LBB1_10;

LBB1_11:
mul.f32 %f48, %f7, %f7;
fma.rn.f32 %f49, %f48, 0fC1CB874B, 0f42259096;
fma.rn.f32 %f50, %f48, %f49, 0fC194EB85;
fma.rn.f32 %f51, %f48, %f50, 0f40206C99;
mul.f32 %f52, %f7, %f51;
fma.rn.f32 %f53, %f48, 0f40485F81, 0fC1A87F78;
fma.rn.f32 %f54, %f48, %f53, 0f41B8AABD;
fma.rn.f32 %f55, %f48, %f54, 0fC1079380;
fma.rn.f32 %f56, %f48, %f55, 0f3F800000;
div.rn.f32 %f60, %f52, %f56;
bra.uni LBB1_12;

LBB1_10:
lg2.approx.f32 %f36, %f6;
mul.f32 %f37, %f36, 0fBF317218;
lg2.approx.f32 %f38, %f37;
mul.f32 %f39, %f38, 0f3F317218;
fma.rn.f32 %f40, %f39, 0f34D49E28, 0f349B0EAC;
fma.rn.f32 %f41, %f39, %f40, 0f3806F590;
fma.rn.f32 %f42, %f39, %f41, 0f39CF3175;
fma.rn.f32 %f43, %f39, %f42, 0f3B7BB21F;
fma.rn.f32 %f44, %f39, %f43, 0f3CE2756C;
fma.rn.f32 %f45, %f39, %f44, 0f3E24A839;
fma.rn.f32 %f46, %f39, %f45, 0f3F79E636;
fma.rn.f32 %f47, %f39, %f46, 0f3EACC996;
neg.f32 %f60, %f47;

LBB1_12:
neg.f32 %f57, %f60;
setp.lt.s32 %p8, %r8, 0;
selp.f32 %f58, %f57, %f60, %p8;
mul.wide.u32 %rd10, %r26, 4;
add.s64 %rd11, %rd1, %rd10;
st.global.f32 [%rd11], %f58;
add.s32 %r26, %r26, %r2;
setp.lt.u32 %p9, %r26, %r10;
@%p9 bra LBB1_9;

LBB1_13:
ret;

}


Fatbin elf code:
================
arch = sm_80
code version = [1,7]
producer = <unknown>
host = linux
compile_size = 64bit
